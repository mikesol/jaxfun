{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from parallelism import Parallelism\n",
    "from contextlib import nullcontext\n",
    "import logging\n",
    "import flax.linen as nn\n",
    "from fork_on_parallelism import fork_on_parallelism\n",
    "import yaml\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "from types import SimpleNamespace\n",
    "import local_env\n",
    "import time\n",
    "\n",
    "IS_CPU = local_env.parallelism == Parallelism.NONE\n",
    "if IS_CPU:\n",
    "    print(\"no gpus found\")\n",
    "    os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
    "\n",
    "import subprocess\n",
    "from typing import Any\n",
    "from flax import struct\n",
    "from transformer import TransformerNetwork\n",
    "from clu import metrics\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "import flax.jax_utils as jax_utils\n",
    "import numpy as np\n",
    "import flax.linen as nns\n",
    "import math\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import jax\n",
    "from data import make_data_16\n",
    "import orbax.checkpoint\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from jax.lax import with_sharding_constraint\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "def checkpoint_walker(ckpt):\n",
    "    def _cmp(i):\n",
    "        try:\n",
    "            o = jax.device_get(i)\n",
    "            return o\n",
    "        except Exception as e:\n",
    "            return i\n",
    "\n",
    "    return jax.tree_map(_cmp, ckpt)\n",
    "\n",
    "\n",
    "PRNGKey = jax.Array\n",
    "\n",
    "\n",
    "def trim_batch(tensor, batch_size):\n",
    "    \"\"\"\n",
    "    Truncates the tensor to the largest multiple of batch_size.\n",
    "\n",
    "    Parameters:\n",
    "    tensor (jax.numpy.ndarray): The input tensor with a leading batch dimension.\n",
    "    batch_size (int): The batch size to truncate to.\n",
    "\n",
    "    Returns:\n",
    "    jax.numpy.ndarray: The truncated tensor.\n",
    "    \"\"\"\n",
    "    # Get the size of the leading dimension (batch dimension)\n",
    "    B, T, C = tensor.shape\n",
    "    if (B < batch_size) or (T == 0) or (C == 0):\n",
    "        return None\n",
    "\n",
    "    # Calculate the size of the truncated dimension\n",
    "    truncated_size = (B // batch_size) * batch_size\n",
    "\n",
    "    # Truncate the tensor\n",
    "    truncated_tensor = tensor[:truncated_size]\n",
    "\n",
    "    return truncated_tensor\n",
    "\n",
    "\n",
    "RESTORE = None\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    loss: metrics.Average.from_output(\"loss\")\n",
    "\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    key: Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng: PRNGKey, dropout_key: PRNGKey, x, module, tx) -> TrainState:\n",
    "    print(\"creating train state\", rng.shape, x.shape)\n",
    "    variables = module.init(rng, x, x, train=False)\n",
    "    params = variables[\"params\"]\n",
    "    return TrainState.create(\n",
    "        apply_fn=module.apply,\n",
    "        params=params,\n",
    "        tx=tx,\n",
    "        key=dropout_key,\n",
    "        metrics=Metrics.empty(),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_step(state, input, target, dropout_key):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    dropout_train_key = jax.random.fold_in(key=dropout_key, data=state.step)\n",
    "\n",
    "    def loss_fn(params):\n",
    "        pred = state.apply_fn(\n",
    "            {\"params\": params},\n",
    "            input[:, :-1, :],\n",
    "            target[:, :-1, :],\n",
    "            train=True,\n",
    "            rngs={\"dropout\": dropout_train_key},\n",
    "        )\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            pred, jnp.squeeze(target[:, 1:, :], axis=-1)\n",
    "        ).mean()\n",
    "        return loss\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def _replace_metrics(state):\n",
    "    return state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "\n",
    "def do_inference(state, input, w_size):\n",
    "    B, T, C = input.shape\n",
    "    input_ = input\n",
    "    assert C == 1\n",
    "\n",
    "    # first, make input into dilated patches with w_size padding on both sides\n",
    "    # then, make a carry with the previous prediction that is w_size long, initializing to 0\n",
    "    # scan over the input\n",
    "    input = jax.lax.conv_general_dilated_patches(\n",
    "        jnp.transpose(input, (0, 2, 1)),\n",
    "        filter_shape=(w_size,),\n",
    "        window_strides=(1,),\n",
    "        padding=((w_size, w_size),),\n",
    "    )\n",
    "    # (batch, w_size, seq)\n",
    "    input = jnp.reshape(input, (B, w_size, -1))\n",
    "    # (seq, batch, w_size)\n",
    "    input = jnp.transpose(input, (2, 0, 1))\n",
    "    # (batch, seq, C)\n",
    "    output = jnp.zeros((B, T, C), dtype=jnp.int32)\n",
    "\n",
    "    # (b, t, c) (b, w_size)\n",
    "    def _loop(c, x):\n",
    "        o = state.apply_fn(\n",
    "            {\"params\": state.params},\n",
    "            jnp.reshape(x, (B, w_size, 1)),\n",
    "            c[:, -w_size:, :],\n",
    "            train=False,\n",
    "        )\n",
    "        # output is B, T, 1\n",
    "        # o is (B, T, Logits)\n",
    "        c = jnp.concatenate(\n",
    "            [\n",
    "                output[:, 1:, :],\n",
    "                jnp.reshape(\n",
    "                    jnp.argmax(o[:, -1:, :], axis=-1).astype(jnp.int32), (B, 1, C)\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        return c, c[:, -1, :]\n",
    "\n",
    "    # (seq, b, c)\n",
    "    c, _ = jax.lax.scan(_loop, output, input)\n",
    "    # print('CC', c.shape, 'OO', output.shape)\n",
    "    # c = jnp.transpose(c, (1, 0, 2))\n",
    "    # print('shapes',c.shape, input_.shape)\n",
    "    assert c.shape == input_.shape\n",
    "    return c\n",
    "\n",
    "replace_metrics = fork_on_parallelism(jax.jit, jax.pmap)(_replace_metrics)\n",
    "\n",
    "\n",
    "def compute_loss(state, input, target, w_size):\n",
    "    B, T, C = input.shape\n",
    "    # find a way to make jitting practical\n",
    "    # and then we can do the whole shebang\n",
    "    input = input  # input = jnp.pad(input, ((0, 0), (w_size, 0), (0, 0)))\n",
    "    to_loop = 1  # T - 1\n",
    "    output = input[:, :w_size, :]\n",
    "    oo = None\n",
    "    for x in range(to_loop):\n",
    "        o = state.apply_fn(\n",
    "            {\"params\": state.params},\n",
    "            input[:, x : x + w_size, :],\n",
    "            output,\n",
    "            train=False,\n",
    "        )\n",
    "        # output is B, T, 1\n",
    "        # o is (B, T, Logits)\n",
    "        oo = o if x == 0 else jnp.concatenate([oo, o[:, -1:, :]], axis=1)\n",
    "        output = jnp.concatenate(\n",
    "            [output, jnp.expand_dims(jnp.argmax(o[:, -1:, :], axis=-1), axis=-1)],\n",
    "            axis=1,\n",
    "        )[:, 1:, :]\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        oo, jnp.reshape(target[:, 1 : w_size + to_loop], (-1, w_size + to_loop - 1))\n",
    "    ).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def _add_losses_to_metrics(state, loss):\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state\n",
    "\n",
    "\n",
    "add_losses_to_metrics = fork_on_parallelism(jax.jit, jax.pmap)(_add_losses_to_metrics)\n",
    "\n",
    "maybe_replicate = fork_on_parallelism(lambda x: x, jax_utils.replicate)\n",
    "maybe_unreplicate = fork_on_parallelism(lambda x: x, jax_utils.unreplicate)\n",
    "maybe_device_put = fork_on_parallelism(jax.device_put, lambda x, _: x)\n",
    "\n",
    "\n",
    "def mesh_sharding(mesh_, pspec: PartitionSpec) -> NamedSharding:\n",
    "    return NamedSharding(mesh_, pspec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7c/zmz1fhrd5rb0dbwfk2j7j71m0000gn/T/ipykernel_70907/1132639044.py:18: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f\"consisder clearing checkpoint dir first: {checkpoint_dir}\")\n",
      "WARNING:root:consisder clearing checkpoint dir first: checkpoints/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 devices\n",
      "Mesh(device_ids=array([[0]]), axis_names=('data', 'model'))\n",
      "making datasets\n",
      "datasets generated\n",
      "creating train state (2,) (8, 1024, 1)\n",
      "creating train state (2,) (8, 1024, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d6277e1bfd4e6abd538fc54bd2cf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from get_files import FILES\n",
    "\n",
    "    logging.basicConfig(level=logging.WARN)\n",
    "    if local_env.parallelism == Parallelism.PMAP:\n",
    "        if local_env.do_manual_parallelism_setup:\n",
    "            jax.distributed.initialize(\n",
    "                coordinator_address=local_env.coordinator_address,\n",
    "                num_processes=local_env.num_processes,\n",
    "                process_id=local_env.process_id,\n",
    "            )\n",
    "        else:\n",
    "            jax.distributed.initialize()\n",
    "\n",
    "    checkpoint_dir = local_env.checkpoint_dir\n",
    "\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        logging.warn(f\"consisder clearing checkpoint dir first: {checkpoint_dir}\")\n",
    "    else:\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "    options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=2, create=True)\n",
    "    checkpoint_manager = orbax.checkpoint.CheckpointManager(\n",
    "        checkpoint_dir, orbax_checkpointer, options\n",
    "    )\n",
    "\n",
    "    device_len = len(jax.devices())\n",
    "\n",
    "    print(f\"Using {device_len} devices\")\n",
    "\n",
    "    if (device_len != 1) and (device_len % 2 == 1):\n",
    "        raise ValueError(\"not \")\n",
    "\n",
    "    # run = Experiment(\n",
    "    #     api_key=local_env.comet_ml_api_key,\n",
    "    #     project_name=\"jax-transformer\",\n",
    "    # )\n",
    "    ## defaults\n",
    "    _config = {}\n",
    "    _config[\"mesh_x_div\"] = 1\n",
    "    _config[\"seed\"] = 42\n",
    "    _config[\"batch_size\"] = 2**4\n",
    "    _config[\"inference_batch_size\"] = 2**3\n",
    "    _config[\"inference_artifacts_per_batch_per_epoch\"] = (\n",
    "        _config[\"inference_batch_size\"] * 4\n",
    "    )\n",
    "    _config[\"learning_rate\"] = 1e-4\n",
    "    _config[\"epochs\"] = 2**7\n",
    "    _config[\"window_plus_one\"] = 2**10 + 1\n",
    "    _config[\"val_window_plus_one\"] = 2**11 + 1\n",
    "    _config[\"inference_window\"] = 2**15\n",
    "    _config[\"stride\"] = 2**8\n",
    "    _config[\"step_freq\"] = 2**6\n",
    "    _config[\"test_size\"] = 0.1\n",
    "    _config[\"vocab_size\"] = 2**16\n",
    "    _config[\"mask_encoder\"] = True\n",
    "    _config[\"n_embed\"] = 2**10\n",
    "    _config[\"n_heads\"] = 2**5\n",
    "    _config[\"dff\"] = 2**11\n",
    "    _config[\"depth\"] = 2**4\n",
    "    _config[\"dropout_rate\"] = 0.2\n",
    "    with open(local_env.config_file, \"r\") as f:\n",
    "        in_config = yaml.safe_load(f)[\"config\"]\n",
    "        for k, v in in_config.items():\n",
    "            if k not in _config:\n",
    "                raise ValueError(f\"Unknown config key {k}\")\n",
    "        for k, v in _config.items():\n",
    "            if k not in in_config:\n",
    "                raise ValueError(f\"Requires key {k}\")\n",
    "        _config = in_config\n",
    "        _config[\"mesh_x\"] = device_len // _config[\"mesh_x_div\"]\n",
    "        _config[\"mesh_y\"] = _config[\"mesh_x_div\"]\n",
    "        del _config[\"mesh_x_div\"]\n",
    "    # run.log_parameters(_config)\n",
    "    # if local_env.parallelism == Parallelism.PMAP:\n",
    "    #     run.log_parameter(\"run_id\", sys.argv[1])\n",
    "    config = SimpleNamespace(**_config)\n",
    "\n",
    "    device_mesh = None\n",
    "    mesh = None\n",
    "    x_sharding = None\n",
    "    state_sharding = None\n",
    "    old_state_sharding = None\n",
    "    # messshhh\n",
    "    if local_env.parallelism == Parallelism.SHARD:\n",
    "        device_mesh = mesh_utils.create_device_mesh((config.mesh_x, config.mesh_y))\n",
    "        mesh = Mesh(devices=device_mesh, axis_names=(\"data\", \"model\"))\n",
    "        print(mesh)\n",
    "        x_sharding = mesh_sharding(mesh, PartitionSpec(\"data\", None))\n",
    "    ###\n",
    "\n",
    "    len_files = len(FILES)\n",
    "    test_files = (\n",
    "        FILES[: int(len_files * config.test_size)] if not IS_CPU else FILES[0:1]\n",
    "    )\n",
    "    train_files = (\n",
    "        FILES[int(len_files * config.test_size) :] if not IS_CPU else FILES[1:2]\n",
    "    )\n",
    "    print(\"making datasets\")\n",
    "    proto_train_dataset, train_dataset_total = make_data_16(\n",
    "        paths=train_files,\n",
    "        window=config.window_plus_one,\n",
    "        stride=config.stride,\n",
    "    )\n",
    "    proto_test_dataset, test_dataset_total = make_data_16(\n",
    "        paths=test_files,\n",
    "        window=config.val_window_plus_one,\n",
    "        stride=config.stride,\n",
    "    )\n",
    "    proto_inference_dataset, inference_dataset_total = make_data_16(\n",
    "        paths=test_files,\n",
    "        window=config.inference_window,\n",
    "        stride=config.stride,\n",
    "    )\n",
    "    print(\"datasets generated\")\n",
    "    init_rng = jax.random.PRNGKey(config.seed)\n",
    "    init_rng, dropout_rng = jax.random.split(init_rng, 2)\n",
    "    onez = jnp.ones([config.batch_size, config.window_plus_one - 1, 1], dtype=jnp.int32)\n",
    "    module = TransformerNetwork(\n",
    "        vocab_size=config.vocab_size,\n",
    "        block_size=config.window_plus_one - 1,\n",
    "        n_embed=config.n_embed,\n",
    "        num_heads=config.n_heads,\n",
    "        dff=config.dff,\n",
    "        depth=config.depth,\n",
    "        dropout_rate=config.dropout_rate,\n",
    "        mask_encoder=config.mask_encoder,\n",
    "    )\n",
    "    tx = optax.adamw(config.learning_rate)\n",
    "\n",
    "    if local_env.parallelism == Parallelism.SHARD:\n",
    "        abstract_variables = jax.eval_shape(\n",
    "            partial(create_train_state, module=module, tx=tx),\n",
    "            init_rng,\n",
    "            dropout_rng,\n",
    "            onez,\n",
    "        )\n",
    "\n",
    "        state_sharding = nn.get_sharding(abstract_variables, mesh)\n",
    "\n",
    "    jit_create_train_state = fork_on_parallelism(\n",
    "        partial(\n",
    "            jax.jit,\n",
    "            static_argnums=(3, 4),\n",
    "            in_shardings=(\n",
    "                (\n",
    "                    mesh_sharding(mesh, None)\n",
    "                    if local_env.parallelism == Parallelism.SHARD\n",
    "                    else None\n",
    "                ),\n",
    "                (\n",
    "                    mesh_sharding(mesh, None)\n",
    "                    if local_env.parallelism == Parallelism.SHARD\n",
    "                    else None\n",
    "                ),\n",
    "                x_sharding,\n",
    "            ),  # PRNG key and x\n",
    "            out_shardings=state_sharding,\n",
    "        ),\n",
    "        partial(jax.pmap, static_broadcasted_argnums=(3, 4)),\n",
    "    )(create_train_state)\n",
    "    init_rng, dropout_rng, loop_rng = jax.random.split(init_rng, 3)\n",
    "    rng_for_train_state = (\n",
    "        init_rng\n",
    "        if local_env.parallelism == Parallelism.SHARD\n",
    "        else jax.random.split(\n",
    "            init_rng, 8\n",
    "        )  ### #UGH we hardcode 8, not sure why this worked before :-/\n",
    "    )\n",
    "    dropout_rng_for_train_state = (\n",
    "        dropout_rng\n",
    "        if local_env.parallelism == Parallelism.SHARD\n",
    "        else jax.random.split(\n",
    "            dropout_rng, 8\n",
    "        )  ### #UGH we hardcode 8, not sure why this worked before :-/\n",
    "    )\n",
    "    state = jit_create_train_state(\n",
    "        rng_for_train_state,\n",
    "        dropout_rng_for_train_state,\n",
    "        onez,\n",
    "        module,\n",
    "        tx,\n",
    "    )\n",
    "\n",
    "    target = {\"model\": state, \"config\": None}\n",
    "\n",
    "    if RESTORE is not None:\n",
    "        CKPT = checkpoint_manager.restore(RESTORE, target)\n",
    "\n",
    "        state = CKPT[\"model\"]\n",
    "\n",
    "    jit_train_step = fork_on_parallelism(\n",
    "        partial(\n",
    "            jax.jit,\n",
    "            in_shardings=(\n",
    "                state_sharding,\n",
    "                x_sharding,\n",
    "                x_sharding,\n",
    "                (\n",
    "                    mesh_sharding(mesh, None)\n",
    "                    if local_env.parallelism == Parallelism.SHARD\n",
    "                    else None\n",
    "                ),\n",
    "            ),\n",
    "            out_shardings=(state_sharding, None),\n",
    "        ),\n",
    "        jax.pmap,\n",
    "    )(train_step)\n",
    "\n",
    "    jit_compute_loss = fork_on_parallelism(\n",
    "        partial(\n",
    "            jax.jit,\n",
    "            static_argnums=(3,),\n",
    "            in_shardings=(state_sharding, x_sharding, x_sharding),\n",
    "        ),\n",
    "        partial(jax.pmap, static_broadcasted_argnums=(3,)),\n",
    "    )(compute_loss)\n",
    "    jit_do_inference = fork_on_parallelism(\n",
    "        partial(\n",
    "            jax.jit,\n",
    "            in_shardings=(state_sharding, x_sharding),\n",
    "            out_shardings=x_sharding,\n",
    "            static_argnums=(2,),\n",
    "        ),\n",
    "        partial(jax.pmap, static_broadcasted_argnums=(2,)),\n",
    "    )(do_inference)\n",
    "    del init_rng  # Must not be used anymore.\n",
    "    for epoch in range(config.epochs):\n",
    "        # ugggh\n",
    "        # commenting out for now\n",
    "        epoch_is_0 = epoch == 0\n",
    "        to_take_in_0_epoch = 104\n",
    "        train_dataset = (\n",
    "            proto_train_dataset\n",
    "            if not epoch_is_0\n",
    "            else proto_train_dataset.take(config.batch_size * to_take_in_0_epoch)\n",
    "        )\n",
    "        test_dataset = (\n",
    "            proto_test_dataset\n",
    "            if not epoch_is_0\n",
    "            else proto_test_dataset.take(config.batch_size * to_take_in_0_epoch)\n",
    "        )\n",
    "        inference_dataset = (\n",
    "            proto_inference_dataset\n",
    "            if not epoch_is_0\n",
    "            else proto_inference_dataset.take(config.batch_size * to_take_in_0_epoch)\n",
    "        )\n",
    "\n",
    "        # log the epoch\n",
    "        # run.log_current_epoch(epoch)\n",
    "        train_dataset.set_epoch(epoch)\n",
    "\n",
    "        # train\n",
    "        train_total = train_dataset_total // config.batch_size\n",
    "        with tqdm(\n",
    "            enumerate(\n",
    "                train_dataset.iter(batch_size=config.batch_size, drop_last_batch=False)\n",
    "            ),\n",
    "            total=train_total if not epoch_is_0 else to_take_in_0_epoch,\n",
    "            unit=\"batch\",\n",
    "        ) as loop:\n",
    "            for batch_ix, batch in loop:\n",
    "                should_use_gen = batch_ix % 2 == 1\n",
    "                input = trim_batch(batch[\"input\"], config.batch_size)\n",
    "                if input is None:\n",
    "                    continue\n",
    "                assert input.shape[1] == config.window_plus_one\n",
    "                input = maybe_replicate(input)\n",
    "                input = maybe_device_put(input, x_sharding)\n",
    "                target = trim_batch(batch[\"target\"], config.batch_size)\n",
    "                if target is None:\n",
    "                    continue\n",
    "                assert target.shape[1] == config.window_plus_one\n",
    "                target = maybe_replicate(target)\n",
    "                with fork_on_parallelism(mesh, nullcontext()):\n",
    "                    loop_rng, new_dropout_rng = jax.random.split(loop_rng, 2)\n",
    "                    dropout_rng_for_train_state = (\n",
    "                        new_dropout_rng\n",
    "                        if local_env.parallelism == Parallelism.SHARD\n",
    "                        else jax.random.split(\n",
    "                            new_dropout_rng, 8\n",
    "                        )  ### #UGH we hardcode 8, not sure why this worked before :-/\n",
    "                    )\n",
    "                    state, loss = jit_train_step(\n",
    "                        state, input, target, dropout_rng_for_train_state\n",
    "                    )\n",
    "\n",
    "                    state = add_losses_to_metrics(state=state, loss=loss)\n",
    "\n",
    "                if batch_ix % config.step_freq == 0:\n",
    "                    metrics = maybe_unreplicate(state.metrics).compute()\n",
    "                    # run.log_metrics({\"train_loss\": metrics[\"loss\"]}, step=batch_ix)\n",
    "                    loop.set_postfix(loss=metrics[\"loss\"])\n",
    "                    state = replace_metrics(state)\n",
    "                    current_time = time.time()\n",
    "                    elapsed_time = current_time - start_time\n",
    "        # temporarily move checkpoint to after the first epoch as it crashes otherwise\n",
    "        if True:\n",
    "            # we test checkpointing early just to make sure it\n",
    "            # works so there aren't any nasty surprises\n",
    "            # checkpoint\n",
    "            ckpt_model = state\n",
    "            # needs to use underscore config\n",
    "            # becuase otherwise it doesn't serialize correctly\n",
    "            ckpt = {\"model\": ckpt_model, \"config\": _config}\n",
    "            if local_env.parallelism == Parallelism.PMAP:\n",
    "                ckpt = checkpoint_walker(ckpt)\n",
    "\n",
    "            CHECK_NAME = (\n",
    "                epoch * train_total\n",
    "                # uncomment when we move back\n",
    "                # + batch_ix\n",
    "                + (RESTORE if RESTORE is not None else 0)\n",
    "            )\n",
    "            checkpoint_manager.save(CHECK_NAME, ckpt)\n",
    "            logging.warning(\n",
    "                f\"saved checkpoint for epoch {epoch} in {os.listdir(checkpoint_dir)}\"\n",
    "            )\n",
    "            try:\n",
    "                subprocess.run(\n",
    "                    f\"gsutil -m rsync -r {os.path.join(checkpoint_dir)} gs://meeshkan-experiments/jax-transformers/{run.id}\",\n",
    "                    check=True,\n",
    "                    shell=True,\n",
    "                    stdout=subprocess.PIPE,\n",
    "                    stderr=subprocess.PIPE,\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                logging.warning(f\"checkpoint artifact did not work {e}\")\n",
    "            start_time = current_time\n",
    "            # hack suggested on https://github.com/google/flax/discussions/1690\n",
    "            # print(state.params)\n",
    "        test_dataset.set_epoch(epoch)\n",
    "        with tqdm(\n",
    "            enumerate(\n",
    "                test_dataset.iter(batch_size=config.batch_size, drop_last_batch=False)\n",
    "            ),\n",
    "            total=(\n",
    "                (test_dataset_total // config.batch_size)\n",
    "                if not epoch_is_0\n",
    "                else to_take_in_0_epoch\n",
    "            ),\n",
    "            unit=\"batch\",\n",
    "        ) as loop:\n",
    "            for batch_ix, batch in loop:\n",
    "                input = maybe_replicate(trim_batch(batch[\"input\"], config.batch_size))\n",
    "                if input is None:\n",
    "                    continue\n",
    "                input = maybe_device_put(input, x_sharding)\n",
    "                target = maybe_replicate(trim_batch(batch[\"target\"], config.batch_size))\n",
    "                if target is None:\n",
    "                    continue\n",
    "                loss = jit_compute_loss(\n",
    "                    state,\n",
    "                    input,\n",
    "                    target,\n",
    "                    config.window_plus_one - 1,\n",
    "                )\n",
    "                state = add_losses_to_metrics(state=state, loss=loss)\n",
    "        metrics = maybe_unreplicate(state.metrics).compute()\n",
    "        # run.log_metrics({\"val_loss\": metrics[\"loss\"]}, step=batch_ix)\n",
    "        state = replace_metrics(state)\n",
    "        # inference\n",
    "        inference_dataset.set_epoch(epoch)\n",
    "        for batch_ix, batch in tqdm(\n",
    "            enumerate(\n",
    "                inference_dataset.take(\n",
    "                    config.inference_artifacts_per_batch_per_epoch\n",
    "                ).iter(batch_size=config.inference_batch_size)\n",
    "            ),\n",
    "            total=config.inference_artifacts_per_batch_per_epoch,\n",
    "        ):\n",
    "            input_ = trim_batch(batch[\"input\"], config.inference_batch_size)\n",
    "            if input_ is None:\n",
    "                continue\n",
    "            target_ = trim_batch(batch[\"target\"], config.inference_batch_size)\n",
    "            if target_ is None:\n",
    "                continue\n",
    "            input = maybe_replicate(input_)\n",
    "            input = maybe_device_put(input, x_sharding)\n",
    "            logging.warning(f\"input shape for inference is is {input.shape}\")\n",
    "            o = jit_do_inference(state, input, config.window_plus_one - 1)\n",
    "            o = maybe_unreplicate(o)\n",
    "            # this will squeeze out the logit dimension\n",
    "            o = jnp.argmax(o, axis=-1)\n",
    "            assert len(o.shape) == 2\n",
    "            # logging.info(f\"shape of batch is {input.shape}\")\n",
    "\n",
    "            for i in range(o.shape[0]):\n",
    "                audy = np.array(o[i])\n",
    "                # vocab to float\n",
    "                audy = audy.astype(np.float32) - 32768\n",
    "                audy = audy / 32768\n",
    "                print(\"prediction dimension\", audy.shape)\n",
    "                # run.log_audio(\n",
    "                #     audy,\n",
    "                #     sample_rate=44100,\n",
    "                #     step=batch_ix,\n",
    "                #     file_name=f\"audio_{epoch}_{batch_ix}_{i}_prediction.wav\",\n",
    "                # )\n",
    "                audy = np.squeeze(np.array(input_[i, :, :1]))\n",
    "                # run.log_audio(\n",
    "                #     audy,\n",
    "                #     sample_rate=44100,\n",
    "                #     step=batch_ix,\n",
    "                #     file_name=f\"audio_{epoch}_{batch_ix}_{i}_input.wav\",\n",
    "                # )\n",
    "                audy = np.squeeze(np.array(target_[i]))\n",
    "                # run.log_audio(\n",
    "                #     audy,\n",
    "                #     sample_rate=44100,\n",
    "                #     step=batch_ix,\n",
    "                #     file_name=f\"audio_{epoch}_{batch_ix}_{i}_target.wav\",\n",
    "                # )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
